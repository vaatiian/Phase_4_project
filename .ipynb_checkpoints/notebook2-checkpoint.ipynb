{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PHASE 4 PROJECT:\n",
    "# DEVELOPMENT AND EVALUATION OF A MACHINE LEARNING-BASED DJIA PREDICTION SYSTEM.\n",
    "\n",
    "| Collaborators        | \n",
    "| -------------------- |\n",
    "| Ian Vaati            |\n",
    "| Sylvia Murithi       |\n",
    "| Bushra Mohammed      |\n",
    "\n",
    "**Project Submission Date:** December 11th, 2023.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BUSINESS UNDERSTANDING."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Executive Summary\n",
    "\n",
    "The stock market is a complex and dynamic system that plays a crucial role in the global economy. Predicting stock market movements can provide valuable insights for investors and financial institutions, enabling them to make informed decisions about investment strategies and risk management. This project aims to develop a machine learning model capable of predicting the short-term movement of the Dow Jones Industrial Average (DJIA), a prominent stock market index, using historical data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Business Problem\n",
    "\n",
    "Accurately forecasting stock market movements is a challenging task due to the inherent volatility and unpredictability of financial markets. Existing forecasting methods often rely on simple technical indicators or subjective analysis, which may not capture the full complexity of market dynamics. A more sophisticated approach is needed to provide accurate and consistent predictions that can inform investment decisions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Business Objectives\n",
    "\n",
    "* **Forecasting Accuracy:** Develop accurate and reliable models to predict future trends in the Dow Jones Industrial Average, allowing stakeholders to anticipate market movements with greater precision.\n",
    "\n",
    "* **Risk Mitigation:** Provide insights into potential market risks and opportunities, enabling proactive risk management strategies for investors and financial institutions.\n",
    "\n",
    "* **Decision Support:** Equip decision-makers with actionable information derived from predictive models, empowering them to make informed investment decisions and optimize portfolio management.\n",
    "\n",
    "* **Market Intelligence:** Enhance market intelligence by identifying patterns, trends, and key indicators that contribute to a deeper understanding of market dynamics.\n",
    "\n",
    "### Business Benefits\n",
    "\n",
    "Accurate stock market predictions can provide several benefits to investors and financial institutions:\n",
    "\n",
    "* **Improved Investment Decisions:** By understanding the direction of market movements, investors can make more informed decisions about buying, selling, or holding stocks.\n",
    "\n",
    "\n",
    "* **Risk Management:** Accurate predictions can help investors identify potential risks and take appropriate measures to mitigate them.\n",
    "\n",
    "* **Enhanced Financial Planning:** Financial institutions can use stock market predictions to develop more effective investment strategies and risk management plans.\n",
    "\n",
    "### Business Stakeholders\n",
    "The target stakeholders for this project includes:\n",
    "\n",
    "* **Individual Investors:** Individuals seeking to make informed investment decisions based on market predictions.\n",
    "\n",
    "* **Financial Institutions:** Banks and financial organizations aiming to enhance their risk management strategies.\n",
    "\n",
    "* **Financial Analysts:** Analysts who need to forecast market movements for research and reporting purposes.\n",
    "\n",
    "* **Portfolio Managers:** Individuals responsible for managing investment portfolios, seeking tools to optimize performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Deliverables\n",
    "The project will focus on the following:\n",
    "* **Predictive Models:** Develop and deploy machine learning models capable of forecasting future values of the Dow Jones Industrial Average.\n",
    "\n",
    "* **Visualizations and Reports:** Provide visually appealing and informative representations of market trends, predictions, and relevant indicators to aid decision-makers.\n",
    "\n",
    "* **Documentation:** Create comprehensive documentation detailing the project's methodologies, data sources, model selection, and performance evaluation.\n",
    "\n",
    "* **Training and Support:** Offer training sessions and ongoing support to stakeholders on interpreting and utilizing the predictive models effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Success Criteria\n",
    "The project will be considered successful if the following criteria are met:\n",
    "\n",
    "* The developed machine learning model accurately predicts the direction of DJIA movement (up, down, or sideways) for the specified time horizon.\n",
    "\n",
    "* The model consistently outperforms benchmark models, such as a simple moving average.\n",
    "\n",
    "* The web application or API provides a user-friendly interface for obtaining DJIA predictions.\n",
    "\n",
    "By achieving these success criteria, the project will demonstrate the potential of machine learning to provide valuable insights into stock market movements, empowering investors and financial institutions to make informed decisions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methodology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Research Questions\n",
    "\n",
    "1. Can historical data from Dow Joes Industrial Average (DJIA) be used to predict future market trends accurately?\n",
    "\n",
    "2. What key features or indicators contribte significantly to predicting stock market movements?\n",
    "\n",
    "3. Which machine learning algorithms are best suited for predicting DJIA movement?\n",
    "\n",
    "## Hypothesis\n",
    "\n",
    "1. Historical trends and patterns in Dow Jones data can be leveraged to forecast future market trends with reasonable accuracy.\n",
    "\n",
    "2. Technical indicators like moving averages, relative strength index (RSI), and MACD will be crucial features for predicting stock market trends.\n",
    "\n",
    "3. Time series forecasting models like ARIMA, LSTM, and Prophet will outperform basic regression models in predicting stock market movements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Research Design\n",
    "* **Analytical Approach:** Utilize an analytical approach by applying various machine learning models to historical Dow Jones data to forecast future trends.\n",
    "\n",
    "* **Time Series Analysis:** Focus on time series analysis methodologies to capture the sequential nature of stock market data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Description\n",
    "### Data Source:\n",
    "The data for this project was collected from the [investing.com website](https://www.investing.com/indices/us-30-historical-data)\n",
    ", which provides historical data for various financial indices, including the Dow Jones Industrial Average (DJIA). The data covers a period from 01/03/2000 to 11/24/2023 and includes the following attributes:\n",
    "\n",
    "### Data Variables:\n",
    "1. **Date:** Represents the date for which the data is recorded.\n",
    "\n",
    "2. **Close:** Represents the closing price of the DJIA for the specified date.\n",
    "\n",
    "3. **Open:** Represents the opening price of the DJIA for the specified date.\n",
    "\n",
    "4. **High:** Represents the highest price reached by the DJIA during the specified day.\n",
    "\n",
    "5. **Low:** Represents the lowest price reached by the DJIA during the specified day.\n",
    "\n",
    "6. **Volume:** Represents the total trading volume of the DJIA for the specified day.\n",
    "\n",
    "7. **Change %:** Represents "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Import relevant libraries \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "# Ignore all warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.cluster import KMeans  \n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from pmdarima import auto_arima\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.layers import Input\n",
    "from sklearn.metrics import mean_squared_error \n",
    "# from fbprophet import Prophet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and display the dataset\n",
    "df = pd.read_csv('Dow Jones Industrial Average Historical Data.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a copy of the DataFrame \n",
    "df = df.copy() \n",
    "\n",
    "# Displaying the first few rows of the copied DataFrame \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the data types  \n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the data types to the desired datatypes\n",
    "# 1. Convert the date column to datetime datatype\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "\n",
    "# 2. convert the Price, Open, High, Low Columns to float format\n",
    "df['Price'] = df['Price'].str.replace(',', '').astype(float)\n",
    "df['Open'] = df['Open'].str.replace(',', '').astype(float)\n",
    "df['High'] = df['High'].str.replace(',', '').astype(float)\n",
    "df['Low'] = df['Low'].str.replace(',', '').astype(float)\n",
    "\n",
    "# 3. Convert the Vol. column to Float format after converting 'M' (millions) to numeric values.\n",
    "# Remove 'M' and convert to numeric\n",
    "df['Vol.'] = df['Vol.'].str.replace('M', '').astype(float) * 1_000_000  # Multiply by 1 million\n",
    "# Convert to integer (if no fractional values) or float\n",
    "df['Vol.'] = df['Vol.'].astype(float)  \n",
    "\n",
    "#4. Convert the % column to float formart after removing the percentage symbol\n",
    "df['Change %'] = df['Change %'].str.rstrip('%').astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# summary statistics\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking for outliers\n",
    "# Visualize box plots for all columns\n",
    "plt.figure(figsize=(15, 8))\n",
    "sns.boxplot(data=df)\n",
    "plt.title('Box Plots for All Columns')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify and display potential outliers using Tukey's method\n",
    "def identify_outliers_tukey(data):\n",
    "    Q1 = data.quantile(0.25)\n",
    "    Q3 = data.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    outliers = (data < lower_bound) | (data > upper_bound)\n",
    "    return outliers\n",
    "\n",
    "# Create a DataFrame to store outliers\n",
    "outliers_df = pd.DataFrame()\n",
    "\n",
    "# Identify outliers for each column\n",
    "for column in df.columns:\n",
    "    outliers_df[column] = identify_outliers_tukey(df[column])\n",
    "\n",
    "# Display rows with outliers\n",
    "outliers_rows = df[outliers_df.any(axis=1)]\n",
    "outliers_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The outliers in the dataset are left as they are since it may carry valuable information. Extreme stock price movements can be driven by significant events, news, or market conditions. Removing outliers might result in the loss of important information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Time Series Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the closing prices over time\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(df['Date'], df['Price'])\n",
    "plt.title('Dow Jones Industrial Average Closing Prices Over Time')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Closing Price')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore the Distributions and Trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of Closing Prices\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.histplot(df['Price'], bins=30, kde=True)\n",
    "plt.title('Distribution of Closing Prices')\n",
    "plt.xlabel('Closing Price')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "# Trends in Daily Percentage Change\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.lineplot(x=df['Date'], y=df['Change %'])\n",
    "plt.title('Daily Percentage Change in Dow Jones Industrial Average')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Percentage Change')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation Analysis\n",
    "This analysis aids in understanding relationships, identifying multicollinearity, and guiding feature selection for machine learning models. The heatmap provides a visually intuitive representation, facilitating quick interpretation and communication of complex relationships among financial features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix\n",
    "correlation_matrix = df.corr()\n",
    "\n",
    "# Heatmap to visualize correlations\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f')\n",
    "plt.title('Correlation Heatmap')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time-Based Feature Creation and Technical Indicators\n",
    "\n",
    "Additional time-based features are created, such as day of the week, month, and year. The moving averages (MA_50 and MA_200) and daily returns are calculated.\n",
    "Rows with missing values introduced by rolling means are dropped.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create additional time-based features\n",
    "df['day_of_week'] = df['Date'].dt.dayofweek\n",
    "df['month'] = df['Date'].dt.month\n",
    "df['year'] = df['Date'].dt.year\n",
    "\n",
    "# Calculate moving averages\n",
    "df['MA_10'] = df['Price'].rolling(window=10).mean()\n",
    "df['MA_50'] = df['Price'].rolling(window=50).mean()\n",
    "df['MA_200'] = df['Price'].rolling(window=200).mean()\n",
    "\n",
    "# Calculate daily returns\n",
    "df['daily_return'] = df['Price'].pct_change()\n",
    "\n",
    "# Drop rows with missing values introduced by rolling means\n",
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the original price and moving averages\n",
    "plt.figure(figsize=(14, 8))\n",
    "plt.plot(df['Date'], df['Price'], label='Original Price')\n",
    "plt.plot(df['Date'], df['MA_10'], label='MA_10')\n",
    "plt.plot(df['Date'], df['MA_50'], label='MA_50')\n",
    "plt.plot(df['Date'], df['MA_200'], label='MA_200')\n",
    "\n",
    "plt.title('Dow Jones Industrial Average with Moving Averages')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Price')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the graph we can see that the best values to measure the moving average is the 10 days and the 50 days because we still capture trends in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Relative Strength Index\n",
    "The Relative Strength Index (RSI) is a tool in finance that helps traders understand if a market has moved too much in one direction. It shows if prices have gone up a lot (overbought) or gone down a lot (oversold). \n",
    "\n",
    "When RSI is high, it might mean prices are too high,suggesting a possible drop. On the other hand, if RSI is low, it might suggest prices are too low, it indicates a possible increase. \n",
    "\n",
    "RSI gives a number between 0 and 100, with over 70 indicating overbought and under 30 indicating oversold. Traders use RSI to find potential points where prices could change direction. It's a helpful tool to understand market conditions and make smarter trading choices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate daily price changes\n",
    "delta = df['Price']\n",
    "\n",
    "# Identify gains (positive changes) and set losses to 0\n",
    "gain = delta.where(delta > 0, 0)\n",
    "\n",
    "# Identify losses (negative changes) and set gains to 0\n",
    "loss = -delta.where(delta < 0, 0)\n",
    "\n",
    "# Calculate the average gain over a 14-day window\n",
    "avg_gain = gain.rolling(window=14).mean()\n",
    "\n",
    "# Calculate the average loss over a 14-day window\n",
    "avg_loss = loss.rolling(window=14).mean()\n",
    "\n",
    "# Calculate the Relative Strength (RS)\n",
    "rs = avg_gain / avg_loss\n",
    "\n",
    "# Calculate the Relative Strength Index (RSI)\n",
    "df['RSI'] = 100 - (100 / (1 + rs))\n",
    "\n",
    "\n",
    "# Create lag features\n",
    "df['Price_Lag_1'] = df['Price'].shift(1)\n",
    "df['Price_Lag_5'] = df['Price'].shift(5)\n",
    "\n",
    "# Drop rows with NaN values resulting from lag features\n",
    "df = df.dropna()\n",
    "\n",
    "# View the data\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stationarity of the dataset\n",
    "Checking for stationarity is important because it ensures that the statistical properties of the time series, such as mean and variance, remain constant over time, providing a stable foundation for modeling and forecasting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hypothesis:\n",
    "\n",
    "Null Hypothesis (H0): Presence of a unit root thus exhibits a trend or seasonality\n",
    "\n",
    "Alternative Hypothesis (H1): The time series data is stationary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import adfuller\n",
    "#Test for stationarity\n",
    "def test_stationarity(timeseries):\n",
    "    #Determing rolling statistics\n",
    "    rolmean = timeseries.rolling(12).mean()\n",
    "    rolstd = timeseries.rolling(12).std()\n",
    "    #Plot rolling statistics:\n",
    "    plt.plot(timeseries, color='blue',label='Original')\n",
    "    plt.plot(rolmean, color='red', label='Rolling Mean')\n",
    "    plt.plot(rolstd, color='black', label = 'Rolling Std')\n",
    "    plt.legend(loc='best')\n",
    "    plt.title('Rolling Mean and Standard Deviation')\n",
    "    plt.show(block=False)\n",
    "    print(\"Results of dickey fuller test\")\n",
    "    adft = adfuller(timeseries,autolag='AIC')\n",
    "    output = pd.Series(adft[0:4],index=['Test Statistics','p-value','No. of lags used','Number of observations used'])\n",
    "    for key,values in adft[4].items():\n",
    "        output['critical value (%s)'%key] =  values\n",
    "    print(output)\n",
    "test_stationarity(df['Price'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the result, the Augmented Dickey-Fuller (ADF) test statistic is -2.351880, and the corresponding p-value is 0.155781.\n",
    "In this case, the p-value is greater than 0.05, we, therefore, fail to reject the null hypothesis, this suggests that there is evidence that the series has a unit root and exhibits a trend or seasonality.\n",
    "\n",
    "Since the data is non-stationary, we will apply transformations to make it stationary.\n",
    "Common techniques include differencing or taking the logarithm. We will use the differencing method and drop the null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the first-order difference\n",
    "df['Price_diff'] = df['Price'].diff().dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adf_test(timeseries):\n",
    "    print(\"Differenced Series:\")\n",
    "    print(timeseries.head())\n",
    "    \n",
    "    result = adfuller(timeseries.dropna(), autolag='AIC')  # Drop NaN values before the test\n",
    "    print('ADF Statistic:', result[0])\n",
    "    print('p-value:', result[1])\n",
    "    print('Critical Values:', result[4])\n",
    "\n",
    "    if result[1] <= 0.05:\n",
    "        print('Reject the null hypothesis and conclude that the time series is likely stationary.')\n",
    "    else:\n",
    "        print('Fail to reject the null hypothesis and conclude that the time series is likely non-stationary.')\n",
    "# ADF test on differenced data\n",
    "adf_test(df['Price_diff'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the result, the p-value is below the 0.05 significance level. This provides strong evidence against the null hypothesis.\n",
    "We therefore conclude that the first difference time series is stationary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the differenced data\n",
    "plt.figure(figsize=(16,8))\n",
    "plt.plot(df['Date'],df['Price_diff'])\n",
    "plt.title(\"DJIA First Difference\")\n",
    "plt.xlabel('Date', fontsize=18)\n",
    "plt.ylabel('Close Price', fontsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting the data into train-test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the data into train & test data\n",
    "train_data = df[:int(len(df) * 0.8)]\n",
    "test_data = df[int(len(df) * 0.8):]\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.title('DJIA Prices')\n",
    "plt.xlabel('Dates')\n",
    "plt.ylabel('Prices')\n",
    "plt.plot(df['Price_diff'], label='Training Data')\n",
    "plt.plot(test_data['Price_diff'], 'green', label='Testing Data')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Autoregressive Integrated Moving Average (ARIMA)\n",
    "\n",
    "In essence, ARIMA is designed to capture and model the temporal dependencies, trends, and fluctuations present in time series data. \n",
    "The Autoregressive Integrated Moving Average (ARIMA) model is a powerful time series forecasting method that combines autoregression (AR), differencing (I), and moving average (MA) components. The model is denoted as ARIMA(p, d, q), where p represents the lag order or the number of lag observations included in the model, d is the degree of differencing indicating how many times the raw observations undergo differencing to achieve stationarity, and q is the order of the moving average, which signifies the size of the moving average window.\n",
    "\n",
    "In essence, ARIMA is designed to capture and model the temporal dependencies, trends, and fluctuations present in time series data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pmdarima import auto_arima\n",
    "\n",
    "# Fit the auto_arima model\n",
    "model_autoARIMA = auto_arima(train_data['Price'], start_p=0, start_q=0,\n",
    "                              test='adf',\n",
    "                              max_p=3, max_q=3,\n",
    "                              m=1,\n",
    "                              d=None,\n",
    "                              seasonal=False,\n",
    "                              start_P=0,\n",
    "                              D=0,\n",
    "                              trace=True,\n",
    "                              error_action='ignore',\n",
    "                              suppress_warnings=True,\n",
    "                              stepwise=True)\n",
    "\n",
    "print(model_autoARIMA.summary())\n",
    "model_autoARIMA.plot_diagnostics(figsize=(15,8))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the results obtained from Auto Arima, it is clear that best model is determined to be ARIMA(2,1,2) which means p =2,d=1,q=2. This means: p (Order of Autoregression): 2, d (Order of Differencing): 1, q (Order of Moving Average): 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Based on the plots:\n",
    "# Top left : The residual errors appear fluctuate around the mean of zero.\n",
    "\n",
    "# Top Right : The density plot on the top right indicates a normal distribution with a mean of zero.\n",
    "\n",
    "# Bottom Left : The data is normally distributed\n",
    "\n",
    "# Bottom Right : Based on the Correlogram (ACF plot), the residual errors are not autocorrelatel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ARIMA MODEL\n",
    "model_arima = ARIMA(df['Price_diff'], order=(2, 1, 2))\n",
    "fitted_arima = model_arima.fit()\n",
    "print(fitted_arima.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Forecast future values\n",
    "fc = fitted_arima.forecast(steps=len(test_data))\n",
    "\n",
    "# Evaluate the model\n",
    "mse_arima = mean_squared_error(test_data['Price_diff'], fc)\n",
    "rmse_arima = np.sqrt(mse_arima)\n",
    "mae_arima = np.mean(np.abs(test_data['Price_diff'] - fc))\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(f'Mean Squared Error (MSE)(ARIMA): {mse_arima}')\n",
    "print(f'Root Mean Squared Error (RMSE)(ARIMA): {rmse_arima}')\n",
    "print(f'Mean Absolute Error (MAE)(ARIMA): {mae_arima}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering Analysis using KMeans\n",
    "Features (MA_10, MA_50, MA_200, daily_return) are standardized using StandardScaler.\n",
    "KMeans clustering with three clusters is applied to the scaled features.\n",
    "A new 'cluster' column is added to the dataframe to represent the assigned cluster labels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize features for clustering\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaled_features = scaler.fit_transform(df[['MA_10', 'MA_50', 'MA_200', 'daily_return']])\n",
    "\n",
    "# Apply KMeans clustering on scaled features\n",
    "kmeans = KMeans(n_clusters=3)\n",
    "df['cluster'] = kmeans.fit_predict(scaled_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Model (Gradient Boosting) for Classification\n",
    "\n",
    "Features (including cluster labels) and the target variable (binary classification) are defined.\n",
    "Data is split into training and testing sets.\n",
    "Grid search is performed to find the best hyperparameters for the Gradient Boosting Classifier.\n",
    "The model is trained, and predictions are made on the test set.\n",
    "Model performance metrics such as accuracy and classification report are displayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define features and target variable\n",
    "X = df[['MA_50', 'MA_200', 'daily_return', 'day_of_week', 'month', 'year', 'cluster']]\n",
    "y = np.where(df['Price'].shift(-1) > df['Price'], 1, 0)  # 1 if price increases, else 0\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Grid search for hyperparameter tuning\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [3, 4, 5]\n",
    "}\n",
    "\n",
    "gb_classifier = GradientBoostingClassifier(random_state=42)\n",
    "grid_search = GridSearchCV(estimator=gb_classifier, param_grid=param_grid, cv=3, scoring='accuracy')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Calculate accuracy and other metrics\n",
    "accuracy_gb = accuracy_score(y_test, y_pred)\n",
    "classification_rep = classification_report(y_test, y_pred)\n",
    "\n",
    "# Display the best hyperparameters\n",
    "print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
    "\n",
    "# Display model evaluation metrics\n",
    "print(f\"Accuracy: {accuracy_gb}\")\n",
    "print(\"Classification Report:\\n\", classification_rep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model achieves an overall accuracy of 53.31%, which means it correctly predicts the direction of the Dow Jones Industrial Average (DJIA) movement (whether an increase or a decrease) about 53.31% of the time.\n",
    "\n",
    "Precision indicates how well the model performs when it predicts an increase or decrease. The precision for predicting a decrease (Class 0) is 54%, and for predicting an increase (Class 1) is 52%.\n",
    "\n",
    "Recall measures how well the model captures the actual increases or decreases. The recall for predicting a decrease (Class 0) is 74%, and for predicting an increase (Class 1) is 31%.\n",
    "\n",
    "The F1-Score provides a balance between precision and recall. The F1-Score for predicting a decrease (Class 0) is 62%, and for predicting an increase (Class 1) is 39%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print(conf_matrix)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", linewidths=.5)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# True Positive (TP): 184 instances were correctly predicted as positive.\n",
    "# False Positive (FP): 158 instances were incorrectly predicted as positive.\n",
    "# True Negative (TN): 448 instances were correctly predicted as negative.\n",
    "# False Negative (FN): 371 instances were incorrectly predicted as negative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importance Analysis and Visualization\n",
    "Feature importance is extracted from the trained Gradient Boosting Classifier.\n",
    "A bar plot is created to visualize the importance of each feature in the prediction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance analysis\n",
    "feature_importance = best_model.feature_importances_\n",
    "features = X.columns\n",
    "feature_importance_dict = dict(zip(features, feature_importance))\n",
    "\n",
    "# Plotting feature importance\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=list(feature_importance_dict.values()), y=list(feature_importance_dict.keys()))\n",
    "plt.title('Feature Importance')\n",
    "plt.xlabel('Importance')\n",
    "plt.ylabel('Features')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Series Forecasting using Facebook Prophet\n",
    "\n",
    "The dataset is split into training and testing sets for the Facebook Prophet time series forecasting model.\n",
    "The Prophet model is trained on historical data and used to forecast future prices.\n",
    "Mean Squared Error (MSE) is calculated for evaluation.\n",
    "The forecast is visualized using the Prophet plotting functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from fbprophet import Prophet\n",
    "# import matplotlib.pyplot as plt\n",
    "# from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# # Assuming you already have the necessary libraries and DataFrame (df)\n",
    "\n",
    "# # Train-test split\n",
    "# train_size = int(len(df) * 0.8)\n",
    "# prophet_data = df[['Date', 'Price']].rename(columns={'Date': 'ds', 'Price': 'y'})\n",
    "# train_prophet = prophet_data.iloc[:train_size]\n",
    "# test_prophet = prophet_data.iloc[train_size:]\n",
    "\n",
    "# # Fit Prophet model with daily seasonality\n",
    "# prophet_model = Prophet(daily_seasonality=True)\n",
    "# prophet_model.fit(train_prophet)\n",
    "\n",
    "# # Make future dataframe for predictions\n",
    "# future = prophet_model.make_future_dataframe(periods=len(test_prophet), freq='D')\n",
    "\n",
    "# # Forecast with Prophet\n",
    "# prophet_forecast = prophet_model.predict(future)\n",
    "\n",
    "# # Evaluate Prophet performance\n",
    "# mse_prophet = mean_squared_error(test_prophet['y'], prophet_forecast['yhat'].tail(len(test_prophet)))\n",
    "# print(f'Mean Squared Error (Prophet): {mse_prophet}')\n",
    "\n",
    "# # Visualize Prophet predictions\n",
    "# fig = prophet_model.plot(prophet_forecast)\n",
    "\n",
    "# # Include labels for forecasted side\n",
    "# plt.title('Prophet Predictions for DJIA')\n",
    "# plt.xlabel('Date')\n",
    "# plt.ylabel('Closing Price')\n",
    "\n",
    "# # Highlight the forecasted side\n",
    "# plt.axvline(x=test_prophet['ds'].iloc[0], color='red', linestyle='--', label='Train-Test Split')\n",
    "# plt.fill_between(test_prophet['ds'].values, test_prophet['y'].values, color='gray', alpha=0.3, label='Test Data')\n",
    "# plt.legend()\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "# # Evaluate Prophet performance\n",
    "# mse_prophet = mean_squared_error(test_prophet['y'], prophet_forecast['yhat'].tail(len(test_prophet)))\n",
    "# mae_prophet = mean_absolute_error(test_prophet['y'], prophet_forecast['yhat'].tail(len(test_prophet)))\n",
    "# rmse_prophet = np.sqrt(mse_prophet)\n",
    "\n",
    "# print(f'Mean Squared Error (Prophet): {mse_prophet}')\n",
    "# print(f'Mean Absolute Error (Prophet): {mae_prophet}')\n",
    "# print(f'Root Mean Squared Error (Prophet): {rmse_prophet}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Time Series Prediction using Keras\n",
    "\n",
    "The dataset is prepared and normalized for input to the LSTM (Long Short-Term Memory) neural network.\n",
    "Sequences are created using a function (create_sequences).\n",
    "The LSTM model is defined, compiled, and trained on the training data.\n",
    "Predictions are made on the test set and then inverse transformed to the original scale.\n",
    "Mean Squared Error (MSE) is calculated, and predictions are visualized alongside actual prices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import LSTM, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the data is sorted by date\n",
    "df.sort_values(by='Date', inplace=True)\n",
    "\n",
    "# Extract the 'Price' column as the target variable\n",
    "data = df.reset_index()['Price']\n",
    "\n",
    "# Normalize the data\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "data = scaler.fit_transform(np.array(data).reshape(-1, 1))\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_size = int(len(data) * 0.65)\n",
    "test_size = len(data) - train_size\n",
    "train_data, test_data = data[0:train_size, :], data[train_size:len(data), :1]\n",
    "\n",
    "# Function to create a dataset with look back\n",
    "def create_dataset(dataset, time_step=1):\n",
    "    X, Y = [], []\n",
    "    for i in range(len(dataset)-time_step-1):\n",
    "        a = dataset[i:(i+time_step), 0]\n",
    "        X.append(a)\n",
    "        Y.append(dataset[i + time_step, 0])\n",
    "    return np.array(X), np.array(Y)\n",
    "\n",
    "# Reshape the data into X=t, t+1, t+2, t+3 and Y=t+4\n",
    "time_step = 200  \n",
    "X_train, y_train = create_dataset(train_data, time_step)\n",
    "X_test, y_test = create_dataset(test_data, time_step)\n",
    "\n",
    "# Reshape input to be [samples, time steps, features] for LSTM\n",
    "X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
    "X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)\n",
    "\n",
    "# Input layer\n",
    "input_layer = Input(shape=(X_train.shape[1], X_train.shape[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# LSTM layer\n",
    "lstm_layer = LSTM(units=50, return_sequences=True)(input_layer)\n",
    "lstm_layer = LSTM(units=50, return_sequences=True)(lstm_layer)\n",
    "lstm_layer = LSTM(units=50)(lstm_layer)\n",
    "\n",
    "# Dense layer\n",
    "dense_layer = Dense(units=1)(lstm_layer)\n",
    "\n",
    "# Create the model\n",
    "model = Model(inputs=input_layer, outputs=dense_layer)\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Define hyperparameters\n",
    "epochs = 20\n",
    "batch_size = 16\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, verbose=1)\n",
    "\n",
    "# Predictions\n",
    "train_predict = model.predict(X_train)\n",
    "test_predict = model.predict(X_test)\n",
    "\n",
    "# Inverse transform the predictions\n",
    "train_predict = scaler.inverse_transform(train_predict)\n",
    "test_predict = scaler.inverse_transform(test_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate RMSE\n",
    "train_rmse = np.sqrt(mean_squared_error(y_train, train_predict))\n",
    "print(f'Training RMSE: {train_rmse}')\n",
    "\n",
    "test_rmse = np.sqrt(mean_squared_error(y_test, test_predict))\n",
    "print(f'Testing RMSE: {test_rmse}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plotting training set\n",
    "plt.figure(figsize=(15, 6))\n",
    "plt.plot(scaler.inverse_transform(y_train.reshape(-1, 1)), label='Actual Train Data')\n",
    "plt.plot(train_predict, label='Predicted Train Data', color='red')\n",
    "plt.title('Stock Price Prediction - Training Set')\n",
    "plt.xlabel('Time Steps')\n",
    "plt.ylabel('Stock Price')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plotting testing set\n",
    "plt.figure(figsize=(15, 6))\n",
    "plt.plot(scaler.inverse_transform(y_test.reshape(-1, 1)), label='Actual Test Data')\n",
    "plt.plot(test_predict, label='Predicted Test Data', color='red')\n",
    "plt.title('Stock Price Prediction - Testing Set')\n",
    "plt.xlabel('Time Steps')\n",
    "plt.ylabel('Stock Price')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of  Performance of Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison of Model Performance\n",
    "\n",
    "# Create a DataFrame to store model metrics\n",
    "model_comparison = pd.DataFrame(columns=['Model', 'MSE', 'MAE', 'RMSE'])\n",
    "\n",
    "# ARIMA Model Metrics\n",
    "model_comparison = model_comparison.append({\n",
    "    'Model': 'ARIMA',\n",
    "    'MSE': mse_arima,\n",
    "    'MAE': mae_arima,\n",
    "    'RMSE': rmse_arima\n",
    "}, ignore_index=True)\n",
    "\n",
    "# Prophet Model Metrics\n",
    "model_comparison = model_comparison.append({\n",
    "    'Model': 'Prophet',\n",
    "    'MSE': mse_prophet,\n",
    "    'MAE': mae_prophet,\n",
    "    'RMSE': rmse_prophet\n",
    "}, ignore_index=True)\n",
    "\n",
    "# LSTM Model Metrics\n",
    "model_comparison = model_comparison.append({\n",
    "    'Model': 'LSTM',\n",
    "    'MSE': test_rmse,  # Assuming you use test RMSE for LSTM\n",
    "    'MAE': mean_absolute_error(y_test, test_predict),\n",
    "    'RMSE': test_rmse\n",
    "}, ignore_index=True)\n",
    "\n",
    "\n",
    "# Display the model comparison table\n",
    "print(model_comparison)\n",
    "\n",
    "# Visualize the comparison\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='Model', y='RMSE', data=model_comparison, palette='viridis')\n",
    "plt.title('Model Comparison - RMSE')\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('Root Mean Squared Error (RMSE)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Prophet model has significantly higher errors (RMSE, MAE, MAPE) compared to the other models, suggesting that it may not be performing well on your data. The ARIMA and LSTM models seem to have lower errors compared to the prophet model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of Model Training\n",
    "\n",
    "### Gradient Boosting Classifier\n",
    "\n",
    "- **Best Hyperparameters:** {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 100}\n",
    "- **Accuracy:** 0.5443583118001722\n",
    "\n",
    "Classification Report:\n",
    "               precision    recall  f1-score   support\n",
    "\n",
    "           0       0.55      0.74      0.63       606\n",
    "           1       0.54      0.33      0.41       555\n",
    "#### Classification Report:\n",
    "    accuracy                           0.54      1161\n",
    "   macro avg       0.54      0.54      0.52      1161\n",
    "weighted avg       0.54      0.54      0.52      1161\n",
    "\n",
    "### ARIMA Model\n",
    "\n",
    "-  **Root Mean Squared Error (RMSE):** 14688.52\n",
    "\n",
    "\n",
    "### LSTM Model\n",
    "\n",
    "- **Epochs:** 20\n",
    "- **Root Mean Squared Error (RMSE):** 26464.03644681247\n",
    "\n",
    "### Prophet Model\n",
    "\n",
    "- **Root Mean Squared Error (RMSE):** 29738.195598 \n",
    "\n",
    "The Gradient Boosting Classifier achieved an accuracy of 0.5444, the LSTM model was trained for 20 epochs with a final RMSE of 26464, and the Prophet model had a RMSE of 29738. Further evaluation and comparison with other models can provide insights into their performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considering the RMSE, lower values indicate better predictive performance. In this case, the LSTM Model has the lowest MSE among the presented models. Therefore, based on this metric, the LSTM Model appears to be the most suitable choice for predicting the short-term movement of the Dow Jones Industrial Average."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Investor Guidance\n",
    "\n",
    "Investors are encouraged to leverage the predictions from the LSTM Model as a valuable tool in their decision-making processes. Despite the inherent uncertainty in financial markets, the consistently low Root Mean Squared Error (RMSE) of the LSTM Model Model indicates its relative accuracy in forecasting DJIA movements.\n",
    "\n",
    "It is advisable to integrate these predictions with a comprehensive approach that includes both fundamental and technical analyses. The combination of machine learning predictions with traditional methods enhances the depth of insight, aiding investors in making well-informed decisions.\n",
    "\n",
    "Given the dynamic nature of financial markets, investors should maintain a proactive stance. Continuous monitoring and adaptable strategies, responsive to real-time market conditions, are crucial for effective decision-making.\n",
    "\n",
    "Incorporating machine learning predictions into investment strategies provides a catalyst for a more nuanced understanding of potential market trends. This empowers investors to optimize their overall portfolio management, potentially leading to more informed and strategic investment outcomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
